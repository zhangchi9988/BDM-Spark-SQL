{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba587a03",
   "metadata": {},
   "source": [
    "# COMP7095 - Big Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c252c15",
   "metadata": {},
   "source": [
    "## Spark Lab 3: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb5efd",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames, which can also act as a distributed SQL query engine. In this lab, we learn how to manipulate the data using the functions provided by the dataframe and SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e52bd3",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "It is assumed that you have installed Python 3.9.x and created a virtual environment on your computer. Next, we need to perform the following steps for this lab:\n",
    "\n",
    "1. Download the `ipnb version` of this lab and `movie_reviews.tsv` and save them.\n",
    "2. Launch Terminal/Command prompt.\n",
    "3. Start your Spark with Jupyter Notebook:\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0560b3",
   "metadata": {},
   "source": [
    "### Creating DataFrame from RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14095",
   "metadata": {},
   "source": [
    "Everything now is ready, we can go ahead to work on this lab. \n",
    "\n",
    "First, we import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a629ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e92893",
   "metadata": {},
   "source": [
    "We define a function named \"preprocess\" that will be used to split the values (review and sentiment) of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a8922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    values = line.split('\\t')\n",
    "    return values[1], values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586457e",
   "metadata": {},
   "source": [
    "Then, we get the instance of the Spark context and load the data file to create a resilient distributed data (RDD) object. \n",
    "\n",
    "We use the `filter` function to ignore the header row and pass the data to the `preprocess` function. Then, a new RDD object will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "rdd = sc.textFile('data/movie_reviews.tsv')\n",
    "reviews = rdd.filter(lambda x: x != 'review\\tsentiment').map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e1bae",
   "metadata": {},
   "source": [
    "We can check the content of the RDD object using the `take` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4601ff8",
   "metadata": {},
   "source": [
    "With SQLContext, we can create a dataframe from a RDD object. \\\n",
    "<mark>DataFrame = RDD + Schema</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fec535",
   "metadata": {},
   "source": [
    "To define a schema, we need the `StructField` function to describe each column. The syntax of the `StructField` function is: \\\n",
    "\n",
    "`StructField(col_name, col_type, nullable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70666d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('review', StringType()),\n",
    "    StructField('sentiment', StringType())\n",
    "])\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "# df = sqlContext.createDataFrame(reviews, schema)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(reviews, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4499b63",
   "metadata": {},
   "source": [
    "You review the schema attribute of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe64a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a98578",
   "metadata": {},
   "source": [
    "To view the content of the dataframe, we use the `show` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45643d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0beee",
   "metadata": {},
   "source": [
    "### Creating DataFrame from Data File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e4510",
   "metadata": {},
   "source": [
    "Import the required packages and create a SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d359e",
   "metadata": {},
   "source": [
    "Create a schema and use the SQL context to load the data from the file - `movie_reviews.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc148fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('review', StringType()),\n",
    "    StructField('sentiment', StringType())\n",
    "])\n",
    "\n",
    "df2 = spark.read.csv('data/movie_reviews.tsv', header=True, schema=schema, sep='\\t')\n",
    "df2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286f7cf",
   "metadata": {},
   "source": [
    "User Define Function (UDF) allows us to create new columns based on the existing columns. \n",
    "For example, we want new columns to: \\\n",
    "- present the length of the review\n",
    "- use Boolean (True/False) to present the sentiment of the review\n",
    "- present how many \"funny\" included in the review\n",
    "- present how many \"terrible\" include in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "length = udf(lambda x: len(x))\n",
    "pos = udf(lambda x: x == 'positive')\n",
    "funny = udf(lambda r, s: r.count('funny') if s == 'positive' else 0)\n",
    "terrible = udf(lambda r, s: r.count('terrible') if s == 'negative' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da789a12",
   "metadata": {},
   "source": [
    "Use the UDFs to create new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('length', length('review'))\n",
    "df2 = df2.withColumn('positive', pos('sentiment'))\n",
    "df2 = df2.withColumn('funny', funny('review', 'sentiment'))\n",
    "df2 = df2.withColumn('terrible', terrible('review', 'sentiment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fafdb",
   "metadata": {},
   "source": [
    "We can also delete the unwanted column by using the `drop` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop('sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbef22d",
   "metadata": {},
   "source": [
    "Let's see what is the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3592983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ef837",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Spark provides an important feature to cache intermediate data and provide significant performance improvement while running multiple queries on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514ac05",
   "metadata": {},
   "source": [
    "By default, the dataframe is not cached. We can check its status through the `is_cached` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd40c77",
   "metadata": {},
   "source": [
    "To cache the data, we simply call the cache function of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fa02f",
   "metadata": {},
   "source": [
    "We can verify it by checking the `is_cached` attribute again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae3cae",
   "metadata": {},
   "source": [
    "We can remove the cache by using the `unpersist` function.\n",
    "\n",
    "`df2.unpersist()`\n",
    "\n",
    "Of course, we want to keep using the caching for the following parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25560a0a",
   "metadata": {},
   "source": [
    "### Data Exploring\n",
    "\n",
    "DataFrame provides different functions for retrieving data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cb1fa",
   "metadata": {},
   "source": [
    "#### Ordering\n",
    "We change the display order using the `orderBy` function. For example, sort by the \"positive\" column in ascending order.\n",
    "\n",
    "Note that `ascending=True` means sort in ascending order; and `ascending=False` means sort in decending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.orderBy('positive', ascending=True).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a397d",
   "metadata": {},
   "source": [
    "#### Your task 1: Please sort df2 according to 'negative' in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2f5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9469bd17",
   "metadata": {},
   "source": [
    "#### Ordering by Multiple Columns\n",
    "We can sort the data by multiple columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.orderBy(['positive','terrible'], ascending=[True, False]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b84512",
   "metadata": {},
   "source": [
    "#### Selecting Columns\n",
    "The `select` function allows us to select which columns we want to display. For example, we want to have \"review\", \"positive\", and \"terrible\" columns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(['review', 'positive', 'terrible']).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8a59d",
   "metadata": {},
   "source": [
    "#### Your task 2: Please list df2 by 'review', 'length' and 'funny'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a087ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d29d8434",
   "metadata": {},
   "source": [
    "#### Adding Conditions\n",
    "With the `where` function, we can specify the condition for data retrieval. For example, we want the negative reviews with more than 3000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f90580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select('review', 'positive', 'length').where('positive = false and length > 3000').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406618a3",
   "metadata": {},
   "source": [
    "#### Aggregate\n",
    "With the `agg` (aggregate) functions, we can find the `min`, `max`, `avg`, `stddev`, and `count` from the dataframe.\n",
    "\n",
    "For example, we want to find the maximum number of \"funny\" words in a single review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.agg({'funny':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cb3f8",
   "metadata": {},
   "source": [
    "#### Your task 3: Please find the maximum number of \"terrible\" words in a single review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd558333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae7dc185",
   "metadata": {},
   "source": [
    "#### Grouping\n",
    "Combining with `groupBy` function, we can find the aggregates of different groups. \n",
    "\n",
    "For example, we want to find the average lengths of positive reviews and negative reviews respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a73240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupBy('positive').agg({'length':'avg'}).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876ec13",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "We can also show the simple statistic by using the `summary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05314d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.summary().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360d150",
   "metadata": {},
   "source": [
    "### Using SQL Query\n",
    "We can create a temp view from the dataframe and the view can be used with SQL queries to retrieve the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964548c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc9ff1",
   "metadata": {},
   "source": [
    "Now, let's have a try to get something with a simple SQL query. For example, we want to show the columns including `review` and `length`. So, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6462d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select review, length from movie_reviews').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1832f",
   "metadata": {},
   "source": [
    "### Your task 4\n",
    "Please show the reviews that contains at least 6 \"funny\" words or at least 6 \"terrible\" words. Also, we need them reviews are sorted by the number of the \"funny\" words in decending order and then the number of the \"terrible\" words in decending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58fd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0778d0",
   "metadata": {},
   "source": [
    "We can also get the average lengths of the positive and negative reviews respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select positive, avg(length) as avg_len from movie_reviews group by positive').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f511281",
   "metadata": {},
   "source": [
    "### After using Spark\n",
    "In the end, we should stop the Spark by using the `stop` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c359a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
